{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd4ea5aa84b4b848854b2f49362ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0200542b25e43578eb0a599a0f9624c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c523712134da43e0a6c454dc8154bb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3b974988c44f7d90362f0966ee46ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d38c9bcead94441933b4ee229770da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d642b7faef345008cde1fe72776f1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eba06e2721640288a0881d0223f6fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:1676\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1675\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 1676\u001b[0m \u001b[39mfor\u001b[39;00m key, record \u001b[39min\u001b[39;00m generator:\n\u001b[0;32m   1677\u001b[0m     \u001b[39mif\u001b[39;00m max_shard_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m writer\u001b[39m.\u001b[39m_num_bytes \u001b[39m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\yunusskeete--Carla-COCO-Object-Detection-Dataset\\4c96779d953a8efccbaf42bc3387d51620a44cf55d005c12c61133a47072cfce\\Carla-COCO-Object-Detection-Dataset.py:117\u001b[0m, in \u001b[0;36mCARLA_COCO._generate_examples\u001b[1;34m(self, filepath, split)\u001b[0m\n\u001b[0;32m    115\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mgenerating examples from = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, filepath)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filepath, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    118\u001b[0m     \u001b[39mfor\u001b[39;00m key, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(f):\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\streaming.py:74\u001b[0m, in \u001b[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, download_config\u001b[39m=\u001b[39mdownload_config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:491\u001b[0m, in \u001b[0;36mxopen\u001b[1;34m(file, mode, download_config, *args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m is_local_path(main_hop):\n\u001b[1;32m--> 491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39m(main_hop, mode, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    492\u001b[0m \u001b[39m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yunus\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\d3dd735a494c74bfc9bdc7832b33b2e3ef5bb36d0cb9b889e765af8d2aab10df\\\\train.jsonl'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yunus\\Documents\\Code\\GitHub\\avs\\Carla-COCO-Object-Detection-Dataset\\Carla-COCO-Object-Detection-Dataset-No-Images\\Carla-COCO-Object-Detection-Dataset-No-Images.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images/Carla-COCO-Object-Detection-Dataset-No-Images.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images/Carla-COCO-Object-Detection-Dataset-No-Images.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# ds = datasets.load_dataset(\"yunusskeete/Carla-COCO-Object-Detection-Dataset-No-Images\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images/Carla-COCO-Object-Detection-Dataset-No-Images.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ds \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m\"\u001b[39;49m\u001b[39myunusskeete/Carla-COCO-Object-Detection-Dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images/Carla-COCO-Object-Detection-Dataset-No-Images.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ds\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\load.py:2153\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2150\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   2152\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2153\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   2154\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   2155\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   2156\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   2157\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   2158\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   2159\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2160\u001b[0m )\n\u001b[0;32m   2162\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 954\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    957\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    958\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1718\u001b[0m         dl_manager,\n\u001b[0;32m   1719\u001b[0m         verification_mode,\n\u001b[0;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39mverification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1721\u001b[0m         \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1722\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1723\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:1049\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1045\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m   1047\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1048\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   1052\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1053\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1054\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m   1056\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:1555\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1553\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1554\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1556\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1557\u001b[0m     ):\n\u001b[0;32m   1558\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   1559\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:1712\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1710\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1711\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[1;32m-> 1712\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1714\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "# ds = datasets.load_dataset(\"yunusskeete/Carla-COCO-Object-Detection-Dataset-No-Images\")\n",
    "ds = datasets.load_dataset(\"yunusskeete/Carla-COCO-Object-Detection-Dataset\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = datasets.logging.get_logger(__name__)\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "This dataset contains 1028 images each 640x380 pixels.\n",
    "The dataset is split into 249 test and 779 training examples.\n",
    "Every image comes with MS COCO format annotations.\n",
    "The dataset was collected in Carla Simulator, driving around in autopilot mode in various environments\n",
    "(Town01, Town02, Town03, Town04, Town05) and saving every i-th frame.\n",
    "The labels where then automatically generated using the semantic segmentation information.\n",
    "\"\"\"\n",
    "\n",
    "_HOMEPAGE = \"https://github.com/yunusskeete/Carla-COCO-Object-Detection-Dataset\"\n",
    "\n",
    "_LICENSE = \"MIT\"\n",
    "\n",
    "_URL = \"https://github.com/yunusskeete/Carla-COCO-Object-Detection-Dataset/raw/master/Carla-COCO-Object-Detection-Dataset-No-Images/\"\n",
    "_URLS = {\n",
    "    \"train\": _URL + \"train/train.tar.gz\",\n",
    "    \"test\": _URL + \"test/test.tar.gz\",\n",
    "}\n",
    "\n",
    "_CATEGORIES = [\"automobile\", \"bike\", \"motorbike\", \"traffic_light\", \"traffic_sign\"]\n",
    "\n",
    "def _info(self):\n",
    "    \"\"\"This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\"\"\"\n",
    "\n",
    "    features = datasets.Features(\n",
    "        {\n",
    "            \"image_id\": datasets.Value(\"int64\"),\n",
    "            # \"image\": datasets.Image(),\n",
    "            \"width\": datasets.Value(\"int32\"),\n",
    "            \"height\": datasets.Value(\"int32\"),\n",
    "            \"file_name\": datasets.Value(\"string\"),\n",
    "            \"url\": datasets.Value(\"string\"),\n",
    "            \"objects\": datasets.Sequence(\n",
    "                {\n",
    "                    \"id\": datasets.Sequence(datasets.Value(\"int64\")),\n",
    "                    \"area\": datasets.Sequence(datasets.Value(\"int64\")),\n",
    "                    \"bbox\": datasets.Sequence(datasets.Value(\"float32\"), length=4),\n",
    "                    \"category\": datasets.Sequence(datasets.ClassLabel(names=_CATEGORIES)),\n",
    "                }\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    return datasets.DatasetInfo(\n",
    "        description=_DESCRIPTION,\n",
    "        features=features,\n",
    "        homepage=_HOMEPAGE,\n",
    "        license=_LICENSE,\n",
    "    )\n",
    "\n",
    "def _split_generators(self, dl_manager):\n",
    "    \"\"\"This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\"\"\"\n",
    "\n",
    "    downloaded_files = dl_manager.download_and_extract(_URLS)\n",
    "\n",
    "    return [\n",
    "        datasets.SplitGenerator(\n",
    "            name=datasets.Split.TRAIN,\n",
    "            # These kwargs will be passed to _generate_examples\n",
    "            gen_kwargs={\n",
    "                \"filepath\": os.path.join(downloaded_files[\"train\"], \"train.jsonl\"),\n",
    "                \"split\": \"train\"\n",
    "            }\n",
    "        ),\n",
    "        datasets.SplitGenerator(\n",
    "            name=datasets.Split.TEST,\n",
    "            # These kwargs will be passed to _generate_examples\n",
    "            gen_kwargs={\n",
    "                \"filepath\": os.path.join(downloaded_files[\"test\"], \"test.jsonl\"),\n",
    "                \"split\": \"test\"\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "def _generate_examples(self, filepath, split):\n",
    "    \"\"\"\n",
    "    This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "    The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"generating examples from = %s\", filepath)\n",
    "\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        for key, row in enumerate(f):\n",
    "            data = json.loads(row)\n",
    "            yield key, {\n",
    "                \"image_id\": data[\"image_id\"],\n",
    "                # \"image\": data[\"image\"],\n",
    "                \"width\": data[\"width\"],\n",
    "                \"height\": data[\"height\"],\n",
    "                \"file_name\": data[\"file_name\"],\n",
    "                \"url\": data[\"url\"],\n",
    "                \"objects\": {\n",
    "                    \"id\": data[\"objects\"][\"id\"],\n",
    "                    \"area\": data[\"objects\"][\"area\"],\n",
    "                    \"bbox\": data[\"objects\"][\"bbox\"],\n",
    "                    \"category\": [c-1 for c in data[\"objects\"][\"category\"]]\n",
    "                },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='This dataset contains 1028 images each 640x380 pixels.\\nThe dataset is split into 249 test and 779 training examples.\\nEvery image comes with MS COCO format annotations.\\nThe dataset was collected in Carla Simulator, driving around in autopilot mode in various environments\\n(Town01, Town02, Town03, Town04, Town05) and saving every i-th frame.\\nThe labels where then automatically generated using the semantic segmentation information.\\n', citation='', homepage='https://github.com/yunusskeete/Carla-COCO-Object-Detection-Dataset', license='MIT', features={'image_id': Value(dtype='int64', id=None), 'width': Value(dtype='int32', id=None), 'height': Value(dtype='int32', id=None), 'file_name': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'objects': Sequence(feature={'id': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'area': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'bbox': Sequence(feature=Value(dtype='float32', id=None), length=4, id=None), 'category': Sequence(feature=ClassLabel(names=['automobile', 'bike', 'motorbike', 'traffic_light', 'traffic_sign'], id=None), length=-1, id=None)}, length=-1, id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, dataset_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_info(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d02ee13f9444a6ab0e5e5c9d16cd2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2362b7036b54e53a554419a3e4b1e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SplitGenerator(name='train', gen_kwargs={'filepath': 'C:\\\\Users\\\\yunus\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1ea6e0a232639bfd8409d9e5ab3873942c85fd7c4696b5699e98577c07fd49cb\\\\train.jsonl', 'split': 'train'}, split_info=SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name=None))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DownloadManager\n",
    "\n",
    "dl_manager = DownloadManager()\n",
    "\n",
    "train_generator, test_generator = _split_generators(None, dl_manager)\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\yunus\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1ea6e0a232639bfd8409d9e5ab3873942c85fd7c4696b5699e98577c07fd49cb\\\\train.jsonl',\n",
       " 'train')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(train_generator.gen_kwargs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _generate_examples at 0x000001D26C028F90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_egs = _generate_examples(None, train_generator.gen_kwargs[\"filepath\"], train_generator.gen_kwargs[\"split\"])\n",
    "train_egs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'image_id': 0,\n",
       "  'width': 640,\n",
       "  'height': 380,\n",
       "  'file_name': 'Town01_001020.png',\n",
       "  'url': 'https://github.com/yunusskeete/Carla-COCO-Object-Detection-Dataset/raw/master/images/train/Town01_001020.png',\n",
       "  'objects': {'id': [], 'area': [], 'bbox': [], 'category': []}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "next(train_egs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yunus\\Documents\\Code\\GitHub\\avs\\Carla-COCO-Object-Detection-Dataset\\Carla-COCO-Object-Detection-Dataset-No-Images\\Carla-COCO-Object-Detection-Dataset-No-Images.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images/Carla-COCO-Object-Detection-Dataset-No-Images.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2126\u001b[0m )\n\u001b[0;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2130\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   2131\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2135\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   2139\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   2141\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2142\u001b[0m )\n\u001b[0;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\load.py:1852\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1850\u001b[0m builder_cls \u001b[39m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[39m=\u001b[39mdataset_name)\n\u001b[0;32m   1851\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1852\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1853\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1854\u001b[0m     dataset_name\u001b[39m=\u001b[39mdataset_name,\n\u001b[0;32m   1855\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   1856\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1857\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1858\u001b[0m     \u001b[39mhash\u001b[39m\u001b[39m=\u001b[39m\u001b[39mhash\u001b[39m,\n\u001b[0;32m   1859\u001b[0m     info\u001b[39m=\u001b[39minfo,\n\u001b[0;32m   1860\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1861\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   1862\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1863\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1864\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1865\u001b[0m )\n\u001b[0;32m   1867\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:383\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[39m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_exported_dataset_info()\n\u001b[0;32m    384\u001b[0m     info\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info())\n\u001b[0;32m    385\u001b[0m info\u001b[39m.\u001b[39mbuilder_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:506\u001b[0m, in \u001b[0;36mDatasetBuilder.get_exported_dataset_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exported_dataset_info\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetInfo:\n\u001b[0;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Empty `DatasetInfo` if doesn't exist\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_all_exported_dataset_infos()\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname, DatasetInfo())\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\builder.py:492\u001b[0m, in \u001b[0;36mDatasetBuilder.get_all_exported_dataset_infos\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_all_exported_dataset_infos\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetInfosDict:\n\u001b[0;32m    481\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Empty dict if doesn't exist\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m DatasetInfosDict\u001b[39m.\u001b[39;49mfrom_directory(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_imported_module_dir())\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\info.py:447\u001b[0m, in \u001b[0;36mDatasetInfosDict.from_directory\u001b[1;34m(cls, dataset_infos_dir)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME)):\n\u001b[0;32m    444\u001b[0m     \u001b[39m# this is just to have backward compatibility with dataset_infos.json files\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME), encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    446\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m--> 447\u001b[0m             {\n\u001b[0;32m    448\u001b[0m                 config_name: DatasetInfo\u001b[39m.\u001b[39mfrom_dict(dataset_info_dict)\n\u001b[0;32m    449\u001b[0m                 \u001b[39mfor\u001b[39;00m config_name, dataset_info_dict \u001b[39min\u001b[39;00m json\u001b[39m.\u001b[39mload(f)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    450\u001b[0m             }\n\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\info.py:448\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME)):\n\u001b[0;32m    444\u001b[0m     \u001b[39m# this is just to have backward compatibility with dataset_infos.json files\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME), encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    446\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    447\u001b[0m             {\n\u001b[1;32m--> 448\u001b[0m                 config_name: DatasetInfo\u001b[39m.\u001b[39;49mfrom_dict(dataset_info_dict)\n\u001b[0;32m    449\u001b[0m                 \u001b[39mfor\u001b[39;00m config_name, dataset_info_dict \u001b[39min\u001b[39;00m json\u001b[39m.\u001b[39mload(f)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    450\u001b[0m             }\n\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yunus\\miniconda3\\envs\\gan\\lib\\site-packages\\datasets\\info.py:366\u001b[0m, in \u001b[0;36mDatasetInfo.from_dict\u001b[1;34m(cls, dataset_info_dict)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_dict\u001b[39m(\u001b[39mcls\u001b[39m, dataset_info_dict: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDatasetInfo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    365\u001b[0m     field_names \u001b[39m=\u001b[39m {f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m dataclasses\u001b[39m.\u001b[39mfields(\u001b[39mcls\u001b[39m)}\n\u001b[1;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dataset_info_dict\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m field_names})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "datasets.load_dataset(\"C:/Users/yunus/Documents/Code/GitHub/avs/Carla-COCO-Object-Detection-Dataset/Carla-COCO-Object-Detection-Dataset-No-Images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
